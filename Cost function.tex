\documentclass{article}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage[framemethod=tikz]{mdframed}
\usepackage{lipsum}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{algorithm,algpseudocode}
\usepackage[frozencache=false, cachedir=minted-cache]{minted}
\definecolor{LightGray}{gray}{0.9}
\usepackage{mathtools}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{multirow} % Add this line in your preamble
\newcommand{\pol}[0]{\pmb{\pi}}
\newcommand{\cpol}[0]{\pmb{\mu}}
\definecolor{codebg}{rgb}{0.95,0.95,0.95}
\pagestyle{fancy}
\fancyhf{} % Clear the default header and footer
\lhead{\rightmark} % Use \rightmark for subsection name
\rhead{\thepage}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

\usepackage{graphicx}
\usepackage{mwe}
\usepackage{verbatim}


\begin{document}
	
	\title{Hands on Multi-Agent Reinforcement Learning}
	
	\date{\today}
	
	\maketitle
	
	\tableofcontents  % Table of Contents
	\clearpage  % Start new page
	
	
	\newpage
	\section{Chapter 3: Neural Network}

    TODO: chapter introduce. 
    
    
    \newpage
	\subsection{Cost Function And Gradient Descent}

    In this section, we will explore the core concept of the cost function through a practical example: predicting the click-rate of online products. We'll then learn its mathematical representation. The next question is how to minimize the value of the cost function. This leads us to the gradient descent algorithm, where we'll explore its basic concept and the mathematical techniques for solving it. Don't worry about the mathematics, all of the mathematical operations used in this section are elementary, and you won't need to spend a lot of time on them.
    
    
    \newpage
	\subsubsection{Cost Function}
     We will learn the concept of cost function by solving a prediction problem. The problem is from the online shopping website, As is commonly observed, higher prices for a product lead to fewer clicks, resulting in a lower click rate. Our objective is to construct a model that predicts a product's click rate based on its price. Given a dataset containing various commodities' prices and their corresponding click rates, we can visualize this information in the figure below, with 'x' representing price and 'y' representing click rate.
     
     
     \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.6\textwidth]{figure/price_click_rate_noLine}
        \caption{product's Price And Click Rate}
     \end{figure}
     
     \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.6\textwidth]{figure/price_click_rate}
        \caption{Product's Price And Click Rate}
     \end{figure}
     
     It's important to note that there are several other factors that influence click rates. In practical scenarios, datasets tend to be more intricate. For the sake of clarity, we present a simplified example to facilitate a better understanding of the underlying concept.
     
     From Figure 1,  we intuitively gather that perhaps a straight line could aptly represent the relationship between x and y as in Figure 2.Hence, we hypothesize that the data pattern follows a linear function, specifically $ y = \theta_{1} * x  + \theta_{0}$. In formal terms, theta represents the parameters of this function, which determines accuracy of the model.  So the next step is to choose values for the parameters so that the straight line can best fit the data.

     For a clearer understanding, consider one example $(x_{i}, y_{i})$,   if the predicted value $y = f(x_{i})$ closer to the actual value $y_{i}$, the model is considered more accurate. The gap between the predicted value and the true value is referred to as the error, and we need to measure the error over the entire data set. Therefore, we sum the errors of each data point and square the sum. However, merely summing these errors may result in an inflation of errors with more data points. To counter this, we compute the average error, and for later calculate conveniency, we add divide by 2. 
     
  
     \hspace*{\fill}
     
     Hypothesis:
     $ y = \theta_{1} * x  + \theta_{0}$
     \hspace*{\fill}
     
     \hspace*{\fill}
     
     Cost Function:
     $J\left(\theta_0, \theta_1\right)=\frac{1}{2 m} \sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)^2$
     
     \hspace*{\fill}
     
     The square error function is widely used, and in practice, the cost function can be designed depending on specific condition.

    Our next goal is to find appropriate parameters that minimize the cost function J, making it as small as possible. In the next section, we will discuss how to address this challenge.
    
    
    \newpage
	\subsubsection{Gradient Descent}
    To solve the problem of minimizing the cost function your initial instinct might be to employ differentiation and set the result to zero, a method often used to find minimum or maximum of functions. However, this approach isn't always feasible, especially when dealing with a large number of parameters. Thus, we require a more systematic and versatile approach to identify parameters that minimize the cost function. This led to the discovery of the gradient descent algorithm, a highly effective method. It can be applied to a wide range of functions across various algorithms, making it a cornerstone in the field of machine learning.

Let me first introduce the core idea and procedure of gradient descent, followed by presenting its mathematical formulation.

We start by setting initial values for parameters, which can be arbitrary. Subsequently, we continuously adjust these parameters with the hope of achieving improvement, iterating this process. We need to concern the direction of modification and determine when to stop the process.

The whole process is like standing at the summit of a mountain, aiming to descend as swiftly as possible. Given our limited view, we pivot 360 degrees, identifying the steepest descent direction. We repeat this procedure, advancing one step at a time, until we reach the bottom of the mountain. The gradient descent algorithm employs a similar strategy to pinpoint the location that minimizes the cost function. Mathematically, this is the direction of steepest descent. As for determining when to stop the process, you can set a threshold based on the gap between predicted and true values, or specify a set number of iterations.

It's worth noting that some functions may exhibit complex shapes with multiple low points, as depicted in the Figure 3. Consequently, they may possess more than one potential minimum. Depending on the initial values chosen, the gradient descent algorithm will yield different results, known as local minima. This is a distinctive characteristic of the gradient descent algorithm. In certain scenarios, some algorithms may be need to ascertain the global minimum among all local minima.


    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.6\textwidth]{figure/multi-bottom-func}
        \caption{Multi Local Minimum Function}
     \end{figure}


    Referring back to the earlier example of commodity price and click rate, the mathematical expression for the gradient descent algorithm is presented below:

    \hspace*{\fill}

    $\theta_j:=\theta_j-\alpha \frac{\partial}{\partial \theta_j} J\left(\theta_0, \theta_1\right) \quad \begin{gathered}\text { (simultaneously update } \\ j=0 \text { and } j=1)\end{gathered}$

    \hspace*{\fill}


    For each parameter in each update step, calculated as the original value minus alpha times the derivative value. Here, alpha is the learning rate, which determines the size of each step in the descent. It typically ranges from 0 to 1. A larger alpha implies a more aggressive algorithm, taking larger steps and possibly overshooting the minimum. Conversely, a smaller alpha leads to smaller steps, reducing the chance of missing the minimum, but at the cost of slower convergence. Determining the appropriate learning rate requires several experimentation and experience. The derivative value indicates the steepest descent. Gradient Descent continues this process until the algorithm converges. Convergence means that the parameters have reached a local minimum of the cost function and no further adjustments are required. This implies that the error between predicted and true values is locally minimized.

    It is important to note that the parameters must be updated simultaneously. This can be succinctly expressed by the formula:
     
     \hspace*{\fill}
     
     $\begin{aligned} & \text { temp0 }:=\theta_0-\alpha \frac{\partial}{\partial \theta_0} J\left(\theta_0, \theta_1\right) \\ & \text { temp1 }:=\theta_1-\alpha \frac{\partial}{\partial \theta_1} J\left(\theta_0, \theta_1\right) \\ & \theta_0:=\text { temp0 } \\ & \theta_1:=\text { temp1 }\end{aligned}$
     
     \hspace*{\fill}
     
     
     \newpage
	\subsection{Neural Network}

    The original concept of artificial intelligence was to create a machine with capabilities similar to the human brain. When we look at the human brain, even that of a ten-year-old child, we see a miracle. Not only does it process visual information and understand language and symbols, but it also formulates complex concepts and strategies. It also has a sense of logic and an understanding of time and space. This formidable organ is the driving force behind the progress of human civilization. Given this, if our goal is to construct an intelligent machine, why not draw inspiration from the human brain? Although a baby's brain lacks complete abilities, it is capable of performing tasks as complex as those of an adult brain through training. So perhaps we can design a machine that emulates the infant brain, coupled with a learning algorithm to teach it. Admittedly, our understanding of how the human brain works is also incomplet. The first hypothesis is that we would need to develop thousands of different algorithms to mimic different part of the brain. A second hypothesis suggesting that the brain functions as a vast neural network governed by a single learning algorithm. Neural rewiring experiments showed that a single piece of biological brain tissue could process visual, auditory, and tactile information. For example, both the auditory cortex and the somatosensory cortex showed the ability to learn to see. This suggested the existence of a universal learning algorithm capable of processing different types of data and autonomously acquiring a variety of skills. It was on this second hypothesis that the neural network was founded and eventually became widely accepted. So nerual netwrok experienced fluctuations in popularity throughout the 1980s and 1990s, followed by a decline in the late 1990s. However, recent years have seen a resurgence, driven by the exponential growth in computing power and data resources. Currently, neural networks are widely used in various industries, ranging from image object recognition, recommendation systems, speech recognition, and more.
    
    
    \newpage
	\subsubsection{Biological Neuron}
    Neurons are cells in the brain. Each neuron has dendrites, which serve to receive signals from other neurons. In structure and function, dendrites can be compared to the input wires of a system. They transmit information to the cell body, where it is processed. The processed information is then sent to other neurons through the axon, which is like an output wire. This communication between neurons occurs through pulses of electricity known as spikes, which consume little electricity. The axon connects to the dendrites of other neurons, allowing the transmission of spikes to neighboring cells. The message sent by the neuron can be either the processed result or the same information it originally received.

    In simple terms, this is how the human body works. Our sensory organs, such as the eyes and ears, gather information from the external environment. This information is then transmitted to the brain through the interconnected network of neurons using electrical impulses. The brain then processes this information and sends the calculated results to the muscles and other organs.
    
    
    \newpage
	\subsubsection{Neuron Model}
    We can think of a neuron as a computational unit. It receives information from an input structure, performs computations, and transmits the results to other nodes through an output structure.

    The following diagram illustrates our neuron model. The circle represents a neuron that receives input through input wires, performs computations, and then sends output values through output wires. These output values always pass through an activation function called $h_(\theta)$, which we will discuss later. The parameters of the model are referred to as $\theta$ or weights, and in most cases 'x' denotes input values. It's worth noting that both $\theta$ and x are typically represented as vectors.
    
    
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.6\textwidth]{figure/neural-model}
        \caption{Multi Local Minimum Function}
     \end{figure}
    
    
    \begin{mdframed}[hidealllines=false,backgroundcolor=white!20]
    \textbf{Activation Function}
    
    In the neuron model, the neuron is only capable of linear computation. To import non-linear factors, we include an activation function. This allows the model to accommodate non-linear functions and execute more complex tasks.
    
    \textbf{ Example of activation function}
    
    
        \hspace*{\fill}
        
        Sigmoid Function:
        $f(x)=\sigma(x)=\frac{1}{1+e^{-x}}$
        \hspace*{\fill}
        
    
        \hspace*{\fill}
        
        Tanh Function:
        $f(x)=\tanh (x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$
        \hspace*{\fill}
    
        \hspace*{\fill}
        
        Relu Function:
        $f(x)= \begin{cases}0 & x<0 \\ x & x \geq 0\end{cases}$
        \hspace*{\fill}
    
        \hspace*{\fill}

        Softmax Function:
        $y_i=\operatorname{soft} \max \left(x_i\right)=\frac{e^{x_i}}{\sum_{j=1}^k e^{x_j}}$
        \hspace*{\fill}
    

    \end{mdframed}
    
    
    \begin{comment}
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.6\textwidth]{figure/sigmod}
        \caption{Sigmoid Function}
        \end{figure}
        
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.6\textwidth]{figure/tanh}
        \caption{Tanh Function}
        \end{figure}
        
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.6\textwidth]{figure/relu}
        \caption{Relu Function}
        \end{figure}
        
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.6\textwidth]{figure/softmax}
        \caption{Softmax Function}
        \end{figure}
    \end{comment}

    \begin{figure}[htbp]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=6cm]{figure/sigmod}
    \caption{Sigmoid Function}
    \end{minipage}
    \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=6cm]{figure/tanh}
    \caption{Tanh Function}
    \end{minipage}

    \centering
    \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=6cm]{figure/relu}
    \caption{Relu Function}
    \end{minipage}
    \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=6cm]{figure/softmax}
    \caption{Softmax Function}
    \end{minipage}
    \end{figure}
    
    
    \newpage
	\subsubsection{Feedforward Neural Network}
    
    A number of neurals connected together form a neural network like in Figure 9.
    
    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.6\textwidth]{figure/NN-model}
        \caption{Neural Network}
        \end{figure}
    
    

    The first layer is called the input layer, and the last layer is called the output layer. The layers between the input and output layers are referred to as hidden layers.

    Now we have network model, it's important to understand the mathematical underpinnings of neural networks. Let me explain the notations used in the following explanation. $a^{(j)}_{i}$ denotes the activation of the neuron or unit $i$ in layer $j$. This activation is both computed and output by the unit. For example, $a^{(2)}_1$ denotes the activation of the first unit in layer 2. The symbol $\theta$ represents the parameters of the neural network, and it's a matrix. $\theta^{(j)}$ is a matrix that governs the functional mapping from the $(j-1)$th layer to the $j$th layer. Note that the bias term is ignored in the diagram, bias is a constant term used as $x_0, a_0$, representing a constant factor in functions. Whether to include a bias depends on the specific problem in practice. In the following example, we will explore how to include the bias term.
    

    \begin{figure}[htbp]
        \centering
        \includegraphics[width=0.6\textwidth]{figure/NN-model-active}
        \caption{Neural Network}
        \end{figure}

    The hidden unit is activated by the function $g(x)$. Consequently, its value $a^{(j)}_{i}$ can be calculated by applying the activation function $g(x)$ to a linear combination of the previous layer. In detail, the calculation is as follows:

    $\begin{aligned} a_1^{(2)} & =g\left(\Theta_{10}^{(1)} x_0+\Theta_{11}^{(1)} x_1+\Theta_{12}^{(1)} x_2+\Theta_{13}^{(1)} x_3\right) \\ a_2^{(2)} & =g\left(\Theta_{20}^{(1)} x_0+\Theta_{21}^{(1)} x_1+\Theta_{22}^{(1)} x_2+\Theta_{23}^{(1)} x_3\right) \\ a_3^{(2)} & =g\left(\Theta_{30}^{(1)} x_0+\Theta_{31}^{(1)} x_1+\Theta_{32}^{(1)} x_2+\Theta_{33}^{(1)} x_3\right) \\ h_{\Theta}(x) & =a_1^{(3)}=g\left(\Theta_{10}^{(2)} a_0^{(2)}+\Theta_{11}^{(2)} a_1^{(2)}+\Theta_{12}^{(2)} a_2^{(2)}+\Theta_{13}^{(2)} a_3^{(2)}\right)\end{aligned}$ \\


    \begin{mdframed}[hidealllines=false,backgroundcolor=white!20]
        \textbf{Vectorized implementation}\\
        
        Vector equations give us a more efficient way to perform calculations.
        Set the bias term $x_0 = 1$, $a_0^{(2)}=1$\\
    

        $x=\left[\begin{array}{l}x_0 \\ x_1 \\ x_2 \\ x_3\end{array}\right] \quad z^{(2)}=\left[\begin{array}{c}z_1^{(2)} \\ z_2^{(2)} \\ z_3^{(2)}\end{array}\right]$ \\

        $\begin{aligned} & z^{(2)}=\Theta^{(1)} x \\ & a^{(2)}=g\left(z^{(2)}\right)\end{aligned}$
        
        $
        \begin{aligned}
        & z^{(3)}=\Theta^{(2)} a^{(2)} \\
        & h_{\Theta}(x)=a^{(3)}=g\left(z^{(3)}\right)
        \end{aligned}
        $
        
        \hspace*{\fill}
        
    \end{mdframed}
    
    It's obvious that the process called forward propagation is beacuase the flow of computation: it begins with the input, traverses through the hidden layers, and end on the output layer.

Need to pay attention, the bias term is ignored in the diagram, bias is a constant term used as $x_0, a_0$, representing a constant factor in functions. The decision to include a bias depends on the specific problem at hand. In the subsequent example, we will explore how to incorporate the bias term.

Through the computational steps outlined, we gain insight into how a neural network fits functions. By adjusting the parameters $\theta$, we can generate different functions. These functions represent our hypotheses regarding a specific task.  We will explore this further in the following examples.

The input variables are also called features. A neural network can be used to predict target variables or to classify data. This involves learning patterns from the input features, which are then applied to the prediction or classification task. In some cases, the task may be complex and the data may be large and complicated. How can a neural network extract distinctive and valuable patterns from such data? This is a critical question. Both feature design and network structure need to be considered. But even from our simplified structure above, it is clear that the architecture of a neural network plays a critical role in answering this question. The neural network can reveal more complex and deeper patterns in the data due to the presence of hidden layers. If we were to remove these hidden layers, the structure would essentially be reduced to an activation function. The hidden layers have the ability to perform intricate and deep combinations of the output from the previous layers. By incorporating additional hidden layers, the neural network can go beyond the surface patterns in the data to reveal deeper and more substantial patterns. These are often elusive to human observers when dealing with large data sets.


\begin{mdframed}[hidealllines=true,backgroundcolor=gray!20]
\textbf{Planar Data Classification}

In this example, we will train a single hidden layer neural network to classify the planar dataset, which is a public dataset with the shape of a flower. You can use this example to learn how to implement a neural network using Python: 

\end{mdframed}

\lstset{
   	language=Python, % Set the language for the code
   	backgroundcolor=\color{codebg}, % Set the background color for the code
   	basicstyle=\ttfamily\footnotesize, % Set the basic font style and size
   	frame=single, % Add a frame around the code
   	showstringspaces=false, % Don't show spaces in strings
   	breaklines=true, % Allow line breaks
   	numbers=left, % Show line numbers on the left
   	numberstyle=\tiny\color{gray}, % Style for line numbers
   	commentstyle=\color{green!50!black}, % Style for comments
   	keywordstyle=\color{blue}, % Style for keywords
   	stringstyle=\color{red}, % Style for strings
   	captionpos=b, % Position of the caption
}

\begin{lstlisting}
import numpy as np
import matplotlib.pyplot as plt
from testCases_v2 import *
from public_tests import *
from planar_utils import plot_decision_boundary, sigmoid, load_planar_dataset, load_extra_datasets

# load the dataset 
X, Y = load_planar_dataset()

# Visualize the data:
plt.scatter(X[0, :], X[1, :], c=Y, s=40, cmap=plt.cm.Spectral);

# get data shape 
def layer_sizes(X, Y):
    n_x =  X.shape[0]
    n_h = 4
    n_y = Y.shape[0]
    return (n_x, n_h, n_y)
    

def initialize_parameters(n_x, n_h, n_y):

    W1 = np.random.randn(n_h,n_x) * 0.01
    b1 = np.zeros((n_h,1)) * 0.01
    W2 = np.random.randn(n_y,n_h) * 0.01
    b2 = np.zeros((n_y,1)) * 0.01

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters
    

# forward_propagation
def forward_propagation(X, parameters):

    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']

    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)

    assert(A2.shape == (1, X.shape[1]))

    cache = {"Z1": Z1,
             "A1": A1,
             "Z2": Z2,
             "A2": A2}

    return A2, cache
    
def compute_cost(A2, Y):
    
    m = Y.shape[1] # number of examples
    logprobs = np.multiply(np.log(A2), Y) + np.multiply((1 - Y), np.log(1 - A2))
    cost = - np.sum(logprobs) / m
    cost = float(np.squeeze(cost))    

    return cost
    
    
def backward_propagation(parameters, cache, X, Y):
   
    m = X.shape[1]

    W1 = parameters['W1']
    W2 = parameters['W2']
    
    A1 = cache['A1']
    A2 = cache['A2']
    
    dZ2= A2 - Y
    dW2 = (1 / m) * np.dot(dZ2, A1.T)
    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = np.multiply(np.dot(W2.T, dZ2), 1 - np.power(A1, 2))
    dW1 = (1 / m) * np.dot(dZ1, X.T)
    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)

    grads = {"dW1": dW1,
             "db1": db1,
             "dW2": dW2,
             "db2": db2}

    return grads
    
def update_parameters(parameters, grads, learning_rate=1.2):
   
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
   
    dW1 = grads['dW1']
    db1 = grads['db1']
    dW2 = grads['dW2']
    db2 = grads['db2']
    
    W1 = W1 - learning_rate * dW1
    b1 = b1 - learning_rate * db1
    W2 = W2 - learning_rate * dW2
    b2 = b2 - learning_rate * db2

    parameters = {"W1": W1,
                  "b1": b1,
                  "W2": W2,
                  "b2": b2}

    return parameters
    

# train the model 
def nn_model(X, Y, n_h, num_iterations=10000, print_cost=False):

    np.random.seed(3)
    n_x = layer_sizes(X, Y)[0]
    n_y = layer_sizes(X, Y)[2]

    
    parameters = initialize_parameters(n_x, n_h, n_y)
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']
   

    for i in range(0, num_iterations):

        # values used in backpropagation are stored in a cache.      
        A2, cache = forward_propagation(X, parameters)

        cost = compute_cost(A2, Y)

        grads = backward_propagation(parameters, cache, X, Y)

        parameters = update_parameters(parameters, grads)

        # Print the cost every 1000 iterations
        if print_cost and i % 1000 == 0:
            print ("Cost after iteration %i: %f" % (i, cost))

    return parameters

# using the model to predict
def predict(parameters, X):

    A2, cache = forward_propagation(X, parameters)
    predictions = np.round(A2)

    return predictions 


parameters = nn_model(X, Y, n_h = 4, num_iterations = 10000, print_cost=True)
# Plot the decision boundary
plot_decision_boundary(lambda x: predict(parameters, x.T), X, Y)
# plt.title("Decision Boundary for hidden layer size " + str(4))
    
\end{lstlisting}


\begin{figure}[htbp]
    \centering
    \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=6cm]{figure/planar-dataset}
    \caption{Planar Dataset Visualization}
    \end{minipage}
    \begin{minipage}[t]{0.48\textwidth}
    \centering
    \includegraphics[width=6cm]{figure/nn-classfication-result}
    \caption{Neural Network Classification Result}
    \end{minipage}
    \end{figure}
     
     

\end{document}


